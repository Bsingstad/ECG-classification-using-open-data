\subsection{Data}
The PhysioNet/CinC Challenge 2020 development set consisted of $43101$ ECG-recordings. The datasets were sourced from six subset from four different sources:

\begin{itemize}
    \item The first source is China Physiological Signal Challenge 2018 wich consists of two subsets: The original China Physiological Signal Challenge 2018 dataset \cite{liu_open_2018} and a extra set called China Physiological Signal Challenge Extra. 
    \item The second source is the Physikalisch-Technische Bundesanstalt (PTB) which consist of two subsets. The first one is the PTB Diagnostic \cite{bousseljot_nutzung_2009} and the second subset is PTB-XL \cite{wagner_ptb-xl_2020-1}
    \item The third source is the St. Petersburg Institute of Cardiological Technics (INCART) database \cite{goldberger_physiobank_2000}
    \item  The fourth is the Georgia 12-Lead ECG Challenge Database which is new database and is still not described in any paper other than the PhysioNet/CinC Challenge 2020 paper \cite{alday_classification_2020}.
\end{itemize}
A total of $111$ different diagnoses were present in the total dataset. Each ECG-recording had at least one diagnosis, but some also had more than one diagnosis. The classification of such a data set is considered a multi-label, multi-class classification problem. The goal of PhysioNet / CinC Challenge 2020 was to classify $27$ of the $111$ diagnoses. 



\subsubsection{Splitting of data}

The data were splitted into training (90\%) and validation (10\%) data using 10-fold stratified cross-validation with random state $=42$ \cite{pedregosa_scikit-learn_2011}. The stratification arranged the splitting such that the distribution of diagnoses was the same in both the train and validation data.

\subsection{Preprocessing data}
Initially the diagnosis were encoded with Systematized Nomenclature of Medicine Clinical Terms (SNOMED-CT). The SNOMED-CT codes were decoded into human readable diagnosis and one-hot encoded into a $27$-bit long array. Each of the bits in the array represented one of the $27$ scored diagnosis in the PhysioNet/CinC Challenge 2020 \cite{alday_classification_2020}. The $84$ unscored diagnoses were overlooked and did not represent any change in the 27-bit long label array. The same labels were used in both the CNN-models and the ensemble models, the preprocessing and feature extraction from the ECG were done differently for the CNN model and for the ensemble model.

\subsubsection{Preprocessing for Convolutional models} \hfill \break
All ECG-recordings used by the CNN-models were padded or truncated to a signal length of $5000$ samples. Padding and truncation were done by removing any parts longer than $5000$ samples and adding a tail of $5000 - n$ zeros to any recording of length $n<5000$. The rule-based model on the other hand, which was used in two of eight CNN-models, analyzed the ECG before padding or truncation to $5000$ samples.

The $27$ diagnoses/classes were not balanced and, to prevent the CNN-models to learn more from the diagnosis that occur more frequently in the dataset, a class weight were calculated. The weights were fed to the models during training and gave higher priority to ECGs with rare diagnosis than diagnosis that occur more frequently in the dataset. 

\subsubsection{Preprocessing for the ensemble models}\hfill \break
All ECG recordings were fed into a ECG-featurizer function \cite{bjorn-jostein_singstad_ecg-featurizer_nodate}. The ECG-Featurizer analyzed the ECGs and extracted $112$ features from the ECGs. All of the $112$ features were used in the 12-lead classification while only $63$ were used in the 2-lead classification. Only features that were extracted from lead $II$ and V5 were used in the 2-lead model.

$42 720$ of $43101$ ECGs were successfully featurized pÃ¥ the ECG-featurizer. In addition, $146$ ECG-recordings were removed due to missing values. This gave a total dataset of $42574$ succesfully featurized ECGs to use by the ensemble model.


\subsection{Model architectures}
\subsubsection{CNN architectures}\hfill \break
The CNN architectures used in this study was the same as in \cite{bjorn-jostein_singstad_classifying_2020}. The models are listed in table \ref{tab:modelcombo}. The new contribution in this study is that the $8$ models were scored using cross-validation on the development data. \newline\newline

\vspace{4 mm}
\begin{table}[!htb]
        \centering
        \caption{The eight CNN models developed in \cite{bjorn-jostein_singstad_classifying_2020} and used in this study}
            \scalebox{1.0}{
                \begin{tabular}{lr} \hline\hline
                        Model     \\\hline
                        A) FCN                       \\
                        B) Encoder                   \\
                        C) FCN $||$ age, gender              \\   
                        D) Encoder $||$ age, gender       \\
                        E) Encoder $||$ FCN            \\
                        F) Ecoder $||$ FCN $||$ age, gender     \\
                        G) Encoder $||$ FCN + rule-based model      \\
                        H) Encoder $||$ FCN $||$ age, gender + rule-based model    \\\hline\hline
                \end{tabular}}
                \label{tab:modelcombo}
\end{table}
\vspace{4 mm}
\newline

\subsubsection{Ensemble model architecture}\hfill \break
The architectures trained on the featurized ECG-data were build up using scikit-multilearn \cite{szymanski_scikit-multilearn_2019}. Scikit-multilearn is a library for multi-label classification. The ensemble models built using Label Space Partitioning Classifier \cite{szymanski_how_2016}, Classifier Chain \cite{read_classifier_2009} and random forests. Label Space Partitioning Classifier is a cluster algorithm were each cluster is a separate classification problem. The clusters were selected using a method called fixed label space clusterer. This method lets the developer define the clusters. The clusters in this study were created by iterating over all diagnosis in the training set and for the $n$-th diagnosis there was $m_n$ diagnosis that co-existed with the $n$-th diagnosis across the data set. In total there was $27$ clusters and the size of the clusters, $m_n + 1$, were different for each of the folds in the 10-folded cross-validation. This is illustrated in figure \ref{fig:diagnosis_co_exist}.

\begin{figure}[!hbp]
    \centering
    \includegraphics[width=.6\textwidth]{Figures/Diagnosis_cluster_axis.png}
    \caption{The figure shows how each of the 27 diagnosis co-existed with one or more of the other 26 diagnosis when looking across the whole development dataset. The 27 clusters that was used for the ensemble models used the same index as the co-existing diagnosis.}
    \label{fig:diagnosis_co_exist}
\end{figure}



\subsection{Threshold optimization}
For the CNN-models a threshold for the 27 classes had to be set. New thresholds were set for each fold. A method called Nelder-Mead downhill simplex method \cite{nelder_simplex_1965, virtanen_scipy_2020} was used to optimize the thresholds individually with PhysioNet/CinC Challenge score as the optimization goal. This method can be computational heavy and therefore a subset of the training set were used to optimize the threshold. The subset were determined using a stratified 10-fold and then selecting the first validation fold as the threshold optimization set.

The Nelder-Mead downhill simplex method is used to find the local minimum of a function using the function itself and an initial guess of the optimal variable of the function. In the first fold of the 10-folded cross-validation the initial guess for the Nelder-Mead optimization algorithm was found by giving a 27-element long array values of 1 and multiplied it with a variable that was given values from 0 to 1, with a step size of 0.05. The next folds in In the 10-folded cross-validation used the threshold found by the Nelder-Mead in the previos fold as the initial guess. 

The Ensemble models on the other hand did not need any threshold optimiztion as the output were binarized by the model.





\subsection{Explanation models}
To add explainability to the models used in this study a local interpretable model-agnostic explanation (LIME) was used \cite{lime}. LIME explains the feature importance locally for a given prediction. LIME uses a linear model to explain the prediction of the the mode complex CNNs and ensemble models used in this study. As a proof-of-concept for such explaination models the 12-lead ensemble model and the CNN encoder model were selected to be explained. The ensemble model were explained using a tabular explainer-function.

To explain the CNN model a explainer called recurrent explainer was used. The Encoder model were simplified by swaping the last layer in the Encoder-model with a softmax-layer with two nodes and the dataset were modified such that Normal Sinus Rhythm were equal to Normal class and all other diagnosis were equal to Abnormal class.

\newpage
